/****************************************************************************************
  Project    : Universal Batch JSON Loader (Auto-detect JSON vs JSONL, level-1 flatten)
  Version    : 1.1
  Date       : 2025-06-22
  Author     : Code Copilot (AI)
****************************************************************************************/

proc python;
submit;
import os
import pandas as pd
import json
import traceback
import re
import random

folder = "/sasdata/cdof/pace/internal/QA"   # <-- Change to your folder
saslib = "mylib"
table_prefix = "json_"

input_files = [
    os.path.join(folder, f)
    for f in os.listdir(folder)
    if os.path.isfile(os.path.join(folder, f)) and not f.startswith('.')
]

print(f"Found {len(input_files)} files in {folder}")

def build_table_name(filename):
    name, _ = os.path.splitext(os.path.basename(filename))
    safe = re.sub(r"[^A-Za-z0-9_]", "_", name)
    suffix = str(random.randint(10000, 99999))
    return f"{table_prefix}{safe}_{suffix}"[:32]

def load_json_whole_file(input_file):
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            txt = f.read()
        data = json.loads(txt)
        if isinstance(data, dict):
            return [data]
        elif isinstance(data, list):
            return data
        else:
            return None
    except Exception as e:
        return None

def load_jsonl_per_line(input_file):
    records = []
    with open(input_file, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f, 1):
            l = line.strip()
            if not l:
                continue
            try:
                obj = json.loads(l)
                if isinstance(obj, str):
                    obj = json.loads(obj)
                records.append(obj)
                if i % 1000 == 0:
                    print(f"  Parsed {i} lines...")
            except Exception as e:
                print(f"  Skipping bad line {i}: {l[:80]} Error: {e}")
    return records

summary = []
for input_file in input_files:
    table_name = build_table_name(input_file)
    print(f"\nProcessing {input_file}")
    records = load_json_whole_file(input_file)
    if records is not None:
        print("  Detected pretty-printed JSON (object or array).")
    else:
        print("  Falling back to JSONL (one object per line)...")
        records = load_jsonl_per_line(input_file)
    if not records:
        print(f"  No valid records found in file: {input_file}")
        summary.append({
            "file": input_file,
            "table": table_name,
            "parsed": 0,
            "success": False
        })
        continue

    print(f"  Flattening {len(records)} records (level 1 only)...")
    try:
        df = pd.json_normalize(records, max_level=1)
        # Clean column names for SAS
        def clean_sas_column(col):
            return re.sub(r"[^A-Za-z0-9_]", "_", col)[:32]
        df.columns = [clean_sas_column(col) for col in df.columns]
        print(f"  DataFrame shape: {df.shape}")
        print(f"  First 5 rows:\n{df.head()}")
        print(f"  Saving to SAS table {saslib}.{table_name}")
        SAS.df2sd(df, f"{saslib}.{table_name}", replace=True)
        print(f"  SUCCESS: {saslib}.{table_name} created.")
        summary.append({
            "file": input_file,
            "table": table_name,
            "parsed": len(records),
            "success": True
        })
    except Exception as e:
        print(f"  ERROR flattening or saving {input_file}:")
        print(traceback.format_exc())
        summary.append({
            "file": input_file,
            "table": table_name,
            "parsed": len(records),
            "success": False
        })

print("\n========== SUMMARY REPORT ==========")
print(f"Files processed: {len(summary)}")
for s in summary:
    status = "OK" if s['success'] else "ERROR"
    print(f"  {status}: {s['file']} â†’ {saslib}.{s['table']} | Parsed: {s['parsed']}")
print("=====================================\n")

endsubmit;
run;
