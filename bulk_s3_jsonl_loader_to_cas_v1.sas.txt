/****************************************************************************************
  Project    : Bulk S3 JSONL Loader to CAS
  Version    : 1.0.1
  Date       : 2025-06-22
  Authors    : Code Copilot (AI) with <Your Name>
  Description: 
    - Downloads JSON files from AWS S3 (add .jsonl extension)
    - Parses & combines all into a single wide table (nested as JSON string)
    - Uploads result to SAS Viya CAS
    - Parameters are grouped at the top for easy configuration
****************************************************************************************/

/* --- PARAMETERS: Set these for your job --- */
%let input_folder = /your/target/path/;   /* Local Viya path to save .jsonl files */
%let file_pattern = .jsonl;               /* File extension */
%let caslib = casuser;                    /* CASlib name */
%let out_table = bulk_flat_table;         /* Output CAS table name */

/* (Place your PROC S3 downloads above here, if needed) */

proc python;
submit;
import os
import pandas as pd
import json
import swat

# --------- Variables from SAS macros ---------
input_folder = r"&input_folder"
file_pattern = "&file_pattern"
caslib = "&caslib"
out_table = "&out_table"

print("PYTHON RECEIVED PARAMETERS FROM SAS MACROS:")
print(f"  input_folder = '{input_folder}'")
print(f"  file_pattern = '{file_pattern}'")
print(f"  caslib = '{caslib}'")
print(f"  out_table = '{out_table}'")

def read_multiline_jsonl(path):
    records = []
    buffer = []
    try:
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip().replace('“', '"').replace('”', '"')
                if not line:
                    continue
                buffer.append(line)
                if line.endswith('}'):
                    block = "\n".join(buffer)
                    try:
                        obj = json.loads(block)
                        records.append(obj)
                    except Exception as e:
                        print("SKIPPING record due to parse error:")
                        print(block)
                        print(f"Error: {e}\n---")
                    buffer = []
    except FileNotFoundError:
        print(f"WARNING: File not found: {path} (skipping)")
    return records

def build_df(records):
    all_keys = set()
    rows = []
    for obj in records:
        row = {}
        for k, v in obj.items():
            if isinstance(v, (dict, list)):
                row[k] = json.dumps(v, ensure_ascii=False)
            else:
                row[k] = v
            all_keys.add(k)
        rows.append(row)
    df = pd.DataFrame(rows)
    for k in all_keys:
        if k not in df.columns:
            df[k] = ""
    return df[list(sorted(all_keys))]

all_records = []
for fname in os.listdir(input_folder):
    if fname.endswith(file_pattern):
        fpath = os.path.join(input_folder, fname)
        print(f"Parsing: {fpath}")
        records = read_multiline_jsonl(fpath)
        print(f"  {len(records)} records found.")
        all_records.extend(records)

if all_records:
    df = build_df(all_records)
    print("Sample DataFrame (first 5 rows):")
    print(df.head())
    conn = SAS.cas
    conn.upload_frame(df, casout={'caslib': caslib, 'name': out_table, 'replace': True})
    print(f"All files combined and uploaded to CAS as '{out_table}' in CASlib '{caslib}'.")
else:
    print("No records parsed. Nothing to upload.")

endsubmit;
run;
